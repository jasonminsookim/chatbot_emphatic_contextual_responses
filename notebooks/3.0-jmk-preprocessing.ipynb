{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:28:07.988814Z",
     "start_time": "2019-10-18T00:28:00.194707Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import re\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:28:35.821757Z",
     "start_time": "2019-10-18T00:28:07.991245Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/interim/filtered_reddit_scrape.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:28:35.908837Z",
     "start_time": "2019-10-18T00:28:35.825024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_edited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>presti-ravioli</td>\n",
       "      <td>4802</td>\n",
       "      <td>2019-08-12 20:50:20</td>\n",
       "      <td>depression</td>\n",
       "      <td>Depression for me is a constant feeling of wan...</td>\n",
       "      <td>It’s a constant sense of wanting to go somewhe...</td>\n",
       "      <td>cpidz8</td>\n",
       "      <td>RuneRaccoon</td>\n",
       "      <td>That's a good description of it. Good luck on ...</td>\n",
       "      <td>516.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>presti-ravioli</td>\n",
       "      <td>4802</td>\n",
       "      <td>2019-08-12 20:50:20</td>\n",
       "      <td>depression</td>\n",
       "      <td>Depression for me is a constant feeling of wan...</td>\n",
       "      <td>It’s a constant sense of wanting to go somewhe...</td>\n",
       "      <td>cpidz8</td>\n",
       "      <td>Kavlone</td>\n",
       "      <td>this is the first post i’ve clicked on from th...</td>\n",
       "      <td>497.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>presti-ravioli</td>\n",
       "      <td>4802</td>\n",
       "      <td>2019-08-12 20:50:20</td>\n",
       "      <td>depression</td>\n",
       "      <td>Depression for me is a constant feeling of wan...</td>\n",
       "      <td>It’s a constant sense of wanting to go somewhe...</td>\n",
       "      <td>cpidz8</td>\n",
       "      <td>bennynthejetsss</td>\n",
       "      <td>There’s a name for this phenomenon. It’s calle...</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1565648011.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>presti-ravioli</td>\n",
       "      <td>4802</td>\n",
       "      <td>2019-08-12 20:50:20</td>\n",
       "      <td>depression</td>\n",
       "      <td>Depression for me is a constant feeling of wan...</td>\n",
       "      <td>It’s a constant sense of wanting to go somewhe...</td>\n",
       "      <td>cpidz8</td>\n",
       "      <td>AvoxGirl</td>\n",
       "      <td>Accurate. I saw a post recently that said “Dep...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>presti-ravioli</td>\n",
       "      <td>4802</td>\n",
       "      <td>2019-08-12 20:50:20</td>\n",
       "      <td>depression</td>\n",
       "      <td>Depression for me is a constant feeling of wan...</td>\n",
       "      <td>It’s a constant sense of wanting to go somewhe...</td>\n",
       "      <td>cpidz8</td>\n",
       "      <td>thethirdman3</td>\n",
       "      <td>I’m the EXACT same way. “That place has gotta ...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author  score              created   subreddit  \\\n",
       "0  presti-ravioli   4802  2019-08-12 20:50:20  depression   \n",
       "1  presti-ravioli   4802  2019-08-12 20:50:20  depression   \n",
       "2  presti-ravioli   4802  2019-08-12 20:50:20  depression   \n",
       "3  presti-ravioli   4802  2019-08-12 20:50:20  depression   \n",
       "4  presti-ravioli   4802  2019-08-12 20:50:20  depression   \n",
       "\n",
       "                                               title  \\\n",
       "0  Depression for me is a constant feeling of wan...   \n",
       "1  Depression for me is a constant feeling of wan...   \n",
       "2  Depression for me is a constant feeling of wan...   \n",
       "3  Depression for me is a constant feeling of wan...   \n",
       "4  Depression for me is a constant feeling of wan...   \n",
       "\n",
       "                                                body      id   comment_author  \\\n",
       "0  It’s a constant sense of wanting to go somewhe...  cpidz8      RuneRaccoon   \n",
       "1  It’s a constant sense of wanting to go somewhe...  cpidz8          Kavlone   \n",
       "2  It’s a constant sense of wanting to go somewhe...  cpidz8  bennynthejetsss   \n",
       "3  It’s a constant sense of wanting to go somewhe...  cpidz8         AvoxGirl   \n",
       "4  It’s a constant sense of wanting to go somewhe...  cpidz8     thethirdman3   \n",
       "\n",
       "                                        comment_body  comment_score  \\\n",
       "0  That's a good description of it. Good luck on ...          516.0   \n",
       "1  this is the first post i’ve clicked on from th...          497.0   \n",
       "2  There’s a name for this phenomenon. It’s calle...          217.0   \n",
       "3  Accurate. I saw a post recently that said “Dep...          129.0   \n",
       "4  I’m the EXACT same way. “That place has gotta ...           49.0   \n",
       "\n",
       "  comment_edited  \n",
       "0          False  \n",
       "1          False  \n",
       "2   1565648011.0  \n",
       "3          False  \n",
       "4          False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:28:35.930383Z",
     "start_time": "2019-10-18T00:28:35.911199Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", text)\n",
    "   # text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:29:04.459978Z",
     "start_time": "2019-10-18T00:28:35.933342Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = df['body'].apply(clean_text)\n",
    "responses = df['comment_body'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:33:09.178005Z",
     "start_time": "2019-10-18T00:29:04.462517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    prompts + responses, target_vocab_size=2**13)\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Demonstrate how tokenizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:33:09.201788Z",
     "start_time": "2019-10-18T00:33:09.181036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_index: 6947 \n",
      "\n",
      "prompt: i am a dreamer, a deep thinker. always picturing myself as a worry free, young and happy person enjoying it with someone i truly love. however, being stuck in my head reaching for unrealistic goals knowing l will always be anxious or suffering from bad mental health, and the crippling limits i put on myself will always prevent me from living the life i so desperately wish i could live. and i might just not be good enough for anyone, the fear of being alone forever, which really brings me down. so i continue to play scenes in my head of things i wish to happen, to keep me having hope that maybe just maybe, things will be okay.\n",
      "token_prompt: [1, 17, 7, 2137, 436, 3, 7, 649, 662, 436, 2, 103, 7210, 79, 38, 7, 462, 1220, 3, 1051, 5, 169, 185, 2181, 8, 25, 106, 1, 550, 865, 2, 629, 3, 85, 791, 16, 11, 338, 2536, 18, 7943, 1713, 2624, 669, 364, 39, 103, 30, 340, 29, 777, 54, 206, 260, 1136, 3, 5, 6, 1887, 7024, 1, 233, 31, 79, 39, 103, 2229, 21, 54, 362, 6, 89, 1, 26, 2713, 234, 1, 88, 898, 2, 5, 1, 222, 23, 12, 30, 94, 284, 18, 715, 3, 6, 381, 10, 85, 309, 1689, 3, 138, 49, 1849, 21, 332, 2, 26, 1, 917, 4, 547, 3645, 33, 16, 11, 338, 10, 84, 1, 234, 4, 1000, 3, 4, 174, 21, 129, 213, 14, 214, 23, 4006, 3, 84, 39, 30, 610, 8016]\n",
      "\n",
      "response: i do this a lot and i think it is bad for us. we cannot accept our reality so we dream of a different one. but when we are done with the fantasies we realize what we do not have and who we are not. i am trying to stop doing this. accepting reality and trying to change it if we can is better than day dreaming. read the power of now by eckart tolle.\n",
      "token_response: [1, 19, 22, 7, 130, 5, 1, 67, 8, 9, 206, 18, 527, 2, 69, 76, 832, 247, 1110, 26, 69, 1443, 10, 7, 366, 493, 2, 20, 43, 69, 27, 322, 25, 6, 7358, 8002, 69, 489, 44, 69, 19, 12, 15, 5, 73, 69, 27, 315, 2, 1, 17, 189, 4, 267, 148, 115, 2, 1922, 1110, 5, 189, 4, 463, 8, 32, 69, 47, 9, 167, 114, 102, 7845, 2, 297, 6, 1509, 10, 100, 95, 1794, 1652, 2206, 6357, 8071, 8016]\n"
     ]
    }
   ],
   "source": [
    "rand_ind = random.randint(0, len(prompts))\n",
    "print(f\"random_index: {rand_ind} \\n\\nprompt: {prompts[rand_ind]}\\ntoken_prompt: {tokenizer.encode(prompts[rand_ind])}\"\n",
    "      f\"\\n\\nresponse: {responses[rand_ind]}\\ntoken_response: {tokenizer.encode(responses[rand_ind])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:33:09.225645Z",
     "start_time": "2019-10-18T00:33:09.205557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8228\n",
      "Number of samples: 148630\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Number of samples: {len(prompts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:34:35.786661Z",
     "start_time": "2019-10-18T00:33:09.229981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "\n",
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # tokenize sentence\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "    # check tokenized sentence max length\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "tokenized_prompts, tokenized_responses = tokenize_and_filter(prompts, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:34:35.803359Z",
     "start_time": "2019-10-18T00:34:35.788958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8226,  412,    3, ...,    0,    0,    0],\n",
       "       [8226,  412,    3, ...,    0,    0,    0],\n",
       "       [8226,  412,    3, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [8226,   49,  647, ...,    0,    0,    0],\n",
       "       [8226,   23,  862, ...,    0,    0,    0],\n",
       "       [8226,    1,   15, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T00:34:35.838645Z",
     "start_time": "2019-10-18T00:34:35.811720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8226,    6,  457, ...,    0,    0,    0],\n",
       "       [8226, 1858, 8002, ...,    0,    0,    0],\n",
       "       [8226, 1381,    2, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [8226, 3720,    4, ...,    0,    0,    0],\n",
       "       [8226,  425, 8016, ...,    0,    0,    0],\n",
       "       [8226,    1,   19, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T02:56:13.147651Z",
     "start_time": "2019-10-17T02:56:10.922695Z"
    }
   },
   "outputs": [],
   "source": [
    "# save constant values\n",
    "constants = {'START_TOKEN': START_TOKEN, 'END_TOKEN': END_TOKEN, 'VOCAB_SIZE': VOCAB_SIZE}\n",
    "pickle.dump(constants, open(\"../data/processed/constants.p\", \"wb\")) # pickle prompts\n",
    "\n",
    "# save the tokenized prompts, responses, and tokenizer\n",
    "pickle.dump(tokenized_prompts, open(\"../data/processed/tok_prompts.p\", \"wb\"))\n",
    "pickle.dump(tokenized_responses, open(\"../data/processed/tok_responses.p\", \"wb\")) \n",
    "pickle.dump(tokenizer, open(\"../data/processed/tokenizer.p\", \"wb\")) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
